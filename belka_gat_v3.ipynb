{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary packages\n!pip install rdkit\n!pip install duckdb\n!pip install pandas networkx\n!pip install torch\n!pip install torch-geometric\n\n# Import libraries\nimport numpy as np \nimport pandas as pd \nimport duckdb\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GATConv, global_mean_pool, GCNConv, GINConv\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport itertools\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T16:18:27.079590Z","iopub.execute_input":"2024-06-12T16:18:27.080331Z","iopub.status.idle":"2024-06-12T16:20:00.082644Z","shell.execute_reply.started":"2024-06-12T16:18:27.080266Z","shell.execute_reply":"2024-06-12T16:20:00.081266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/leash-BELKA/train.parquet'\ntest_path = '/kaggle/input/leash-BELKA/test.csv'\n\ncon = duckdb.connect()\ndf = con.query(f\"\"\"(SELECT * FROM parquet_scan('{train_path}') WHERE binds = 0 ORDER BY random() LIMIT 30000) \n                   UNION ALL \n                   (SELECT * FROM parquet_scan('{train_path}') WHERE binds = 1 ORDER BY random() LIMIT 30000)\"\"\").df()\ncon.close()\n\ndf = df.drop(['buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles'], axis=1)\n\ntest_df = pd.read_csv(test_path)\ntest_df = test_df.drop(['buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles'], axis=1)\nprint(test_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:22:38.882421Z","iopub.execute_input":"2024-06-12T16:22:38.882942Z","iopub.status.idle":"2024-06-12T16:23:47.316353Z","shell.execute_reply.started":"2024-06-12T16:22:38.882903Z","shell.execute_reply":"2024-06-12T16:23:47.314768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_colwidth', None)\n# df.sample(n=10)\n\n# import matplotlib.pyplot as plt\n\n# df['binds'].value_counts().plot(kind='bar')\n# plt.title('Training Data Distribution')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T05:09:55.915069Z","iopub.execute_input":"2024-06-11T05:09:55.915594Z","iopub.status.idle":"2024-06-11T05:09:56.244066Z","shell.execute_reply.started":"2024-06-11T05:09:55.915559Z","shell.execute_reply":"2024-06-11T05:09:56.242702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"protein_encoder = LabelEncoder()\nprotein_encoder.fit(['HSA', 'BRD4', 'sEH'])\n\ndef smiles_to_graph(smiles, protein):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return None\n    nodes = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n    edges = [(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()) for bond in mol.GetBonds()]\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    x = torch.tensor(nodes, dtype=torch.float).view(-1, 1)\n    protein_encoded = protein_encoder.transform([protein])[0]\n    protein_feature = torch.tensor([protein_encoded], dtype=torch.float)\n    protein_features = protein_feature.repeat(x.size(0), 1)\n    x = torch.cat([x, protein_features], dim=1)\n    return Data(x=x, edge_index=edge_index)\n\ndf['graph'] = df.apply(lambda row: smiles_to_graph(row['molecule_smiles'], row['protein_name']), axis=1)\ndf = df[df['graph'].notnull()]\ntest_df['graph'] = test_df.apply(lambda row: smiles_to_graph(row['molecule_smiles'], row['protein_name']), axis=1)\ntest_df = test_df[test_df['graph'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:23:47.318539Z","iopub.execute_input":"2024-06-12T16:23:47.318942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MoleculeDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        data = self.dataframe.iloc[idx]\n        graph = data['graph']\n        label = torch.tensor(data['binds'], dtype=torch.long)\n        return graph, label\n\nclass TestMoleculeDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        data = self.dataframe.iloc[idx]\n        graph = data['graph']\n        return graph\n\ndataset = MoleculeDataset(df)\ntest_dataset = TestMoleculeDataset(test_df)\n\n# data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\ntest_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GNN(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, heads=2, layer_type='GAT'):\n        super(GNN, self).__init__()\n        if layer_type == 'GAT':\n            self.conv1 = GATConv(input_dim, hidden_dim, heads=heads)\n            self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads)\n            self.conv3 = GATConv(hidden_dim * heads, output_dim, heads=heads)\n        elif layer_type == 'GCN':\n            self.conv1 = GCNConv(input_dim, hidden_dim)\n            self.conv2 = GCNConv(hidden_dim, hidden_dim)\n            self.conv3 = GCNConv(hidden_dim, output_dim)\n        elif layer_type == 'GIN':\n            nn1 = torch.nn.Sequential(torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(), torch.nn.Linear(hidden_dim, hidden_dim))\n            self.conv1 = GINConv(nn1)\n            nn2 = torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU(), torch.nn.Linear(hidden_dim, hidden_dim))\n            self.conv2 = GINConv(nn2)\n            nn3 = torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU(), torch.nn.Linear(hidden_dim, output_dim))\n            self.conv3 = GINConv(nn3)\n        self.output_dim = output_dim\n\n    def forward(self, data):\n        data = data.to(device)\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = F.relu(self.conv1(x, edge_index))\n        x = F.dropout(x, training=self.training)\n        x = F.relu(self.conv2(x, edge_index))\n        x = F.dropout(x, training=self.training)\n        x = self.conv3(x, edge_index)\n        x = global_mean_pool(x, batch)\n        return F.softmax(x, dim=1)\n\nhidden_dims = [16, 32, 64]\nlearning_rates = [0.01, 0.001, 0.0001]\ndropout_rates = [0.3, 0.5]\nweight_decays = [1e-4, 1e-5]\nlayer_types = ['GAT', 'GCN', 'GIN']\nheads = [2, 4]\n\nhyperparameter_grid = list(itertools.product(hidden_dims, learning_rates, dropout_rates, weight_decays, layer_types, heads))\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=7, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(val_loss, model)\n        elif val_loss > self.best_loss:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_accuracy = 0\nbest_params = None\n\nfor params in hyperparameter_grid:\n    hidden_dim, learning_rate, dropout_rate, weight_decay, layer_type, heads = params\n    print(f\"Testing combination: Hidden Dim: {hidden_dim}, Learning Rate: {learning_rate}, Dropout Rate: {dropout_rate}, Weight Decay: {weight_decay}, Layer Type: {layer_type}, Heads: {heads}\")\n\n    fold_accuracies = []\n\n    for train_idx, val_idx in kf.split(dataset):\n        train_subset = torch.utils.data.Subset(dataset, train_idx)\n        val_subset = torch.utils.data.Subset(dataset, val_idx)\n\n        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n\n        model = GNN(input_dim=2, hidden_dim=hidden_dim, output_dim=2, heads=heads, layer_type=layer_type).to(device)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n        criterion = torch.nn.CrossEntropyLoss()\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n        early_stopping = EarlyStopping(patience=10, verbose=True)\n        \n        model.train()\n        for epoch in range(100):  \n            total_loss = 0\n            for data, labels in train_loader:\n                data, labels = data.to(device), labels.to(device)\n                optimizer.zero_grad()\n                out = model(data)\n                loss = criterion(out, labels)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n            \n            val_loss = 0\n            model.eval()\n            with torch.no_grad():\n                for data, labels in val_loader:\n                    data, labels = data.to(device), labels.to(device)\n                    out = model(data)\n                    loss = criterion(out, labels)\n                    val_loss += loss.item()\n            val_loss /= len(val_loader)\n            scheduler.step(val_loss)\n            early_stopping(val_loss, model)\n            \n            if early_stopping.early_stop:\n                break\n\n        model.load_state_dict(torch.load('checkpoint.pt'))\n\n        def evaluate(model, data_loader):\n            model.eval()\n            correct = 0\n            with torch.no_grad():\n                for data, labels in data_loader:\n                    data, labels = data.to(device), labels.to(device)\n                    out = model(data)\n                    pred = out.argmax(dim=1)\n                    correct += (pred == labels).sum().item()\n            accuracy = correct / len(data_loader.dataset)\n            return accuracy\n\n        val_accuracy = evaluate(model, val_loader)\n        fold_accuracies.append(val_accuracy)\n\n    avg_val_accuracy = np.mean(fold_accuracies)\n    print(f\"Avg Validation Accuracy for params {params}: {avg_val_accuracy}\")\n\n    if avg_val_accuracy > best_accuracy:\n        best_accuracy = avg_val_accuracy\n        best_params = params\n\nprint(f\"Best Hyperparameters: {best_params} with accuracy: {best_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_dim, learning_rate, dropout_rate, weight_decay, layer_type, heads = best_params\nfinal_model = GNN(input_dim=2, hidden_dim=hidden_dim, output_dim=2, heads=heads, layer_type=layer_type).to(device)\nfinal_optimizer = torch.optim.AdamW(final_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\nfinal_criterion = torch.nn.CrossEntropyLoss()\nfinal_scheduler = ReduceLROnPlateau(final_optimizer, mode='min', factor=0.5, patience=5, verbose=True)\nfinal_early_stopping = EarlyStopping(patience=10, verbose=True)\n\nfinal_train_loader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.train()\nfor epoch in range(100):\n    total_loss = 0\n    for data, labels in final_train_loader:\n        data, labels = data.to(device), labels.to(device)\n        final_optimizer.zero_grad()\n        out = final_model(data)\n        loss = final_criterion(out, labels)\n        loss.backward()\n        final_optimizer.step()\n        total_loss += loss.item()\n    final_scheduler.step(total_loss / len(final_train_loader))\n    final_early_stopping(total_loss / len(final_train_loader), final_model)\n    if final_early_stopping.early_stop:\n        break\n\nfinal_model.load_state_dict(torch.load('checkpoint.pt'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions(model, test_data_loader):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for data in test_data_loader:\n            data = data.to(device)\n            out = model(data)\n            probs = out[:, 1]  \n            predictions.extend(probs.cpu().numpy())\n    return predictions\n\ntest_predictions = make_predictions(final_model, test_data_loader)\n\ntest_df['binds'] = test_predictions\n\noutput_df = test_df[['id', 'binds']]\n\noutput_csv_path = '/kaggle/working/test_predictions.csv'\noutput_df.to_csv(output_csv_path, index=False)\n\nprint(f'Saved predictions to {output_csv_path}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_graph(data):\n    G = nx.Graph()\n    x = data.x.cpu().numpy()\n    edge_index = data.edge_index.cpu().numpy()\n    edge_index = data.edge_index.numpy()\n    for i, feature in enumerate(x):\n        G.add_node(i, atom=feature[0], protein=feature[1])\n    for i in range(edge_index.shape[1]):\n        G.add_edge(edge_index[0, i], edge_index[1, i])\n    pos = nx.spring_layout(G)\n    node_labels = nx.get_node_attributes(G, 'atom')\n    nx.draw(G, pos, with_labels=True, labels=node_labels, node_color='lightblue', edge_color='gray')\n    plt.title('Graph Representation of a Molecule')\n    plt.show()\n\nsample_data = df['graph'].sample(1).values[0]\nvisualize_graph(sample_data)\n\nsample_test_data = test_df['graph'].sample(1).values[0]\nvisualize_graph(sample_test_data)","metadata":{},"execution_count":null,"outputs":[]}]}